import math
import random
import numpy
import torch


class NormalGamma():
    """Murphy (2007) section 3.
    Describes a data generating distribution where each point is generated by a normal distribution.
    However you do not know the mean or variance of this distribution. However you do have opinions about
    which means and variances are more likely, NormalGamma is a way of characterising this knowledge.
    NormalGamma(0.0, .01, 1.0, 1.0) gives clusters which have low variance but are widely seperated."""
    def __init__(self, mu, kappa, alpha, beta):
        self.mu = mu
        self.kappa = kappa
        self.alpha = alpha
        self.beta = beta
        self.gamma = torch.distributions.gamma.Gamma(alpha, beta)
        mode_precision = self.gamma.mean  # not quite same, update?
        mode_mu = torch.distributions.normal.Normal(self.mu, 1.0/torch.sqrt(mode_precision*self.kappa)).mean
        self.mode = (mode_mu, mode_precision)

        
    def sample(self):
        precision = self.gamma.sample()
        mu = torch.distributions.normal.Normal(self.mu, 1.0/torch.sqrt(precision*self.kappa)).sample()
        return mu, precision
    
    def log_prob(self, mu, precision):
        log_prob_gamma = self.gamma.log_prob(precision)  # Murphy 2007, equ 63
        log_prob_normal = torch.distributions.normal.Normal(self.mu, 1.0/torch.sqrt(precision*self.kappa)).log_prob(mu)
        return log_prob_gamma + log_prob_normal


def posterior_normal_gamma(prior_normal_gamma, datapoints):
    """myng = ng.NormalGamma(0.0, .01, 1.0, 1.0)
       datapoints = torch.tensor([3.4,7.8,9.8])
       posterior_normal_gamma(myng, datapoints) returns a NormalGamma object."""
    n = datapoints.shape[0]
    if n == 0:
        return prior_normal_gamma
    png = prior_normal_gamma
    mu_n = png.kappa*png.mu + torch.sum(datapoints) / ( png.kappa + n )  # Murphy 2007, equ 86
    kappa_n = png.kappa + n  # Murphy 2007, equ 87
    alpha_n = png.alpha = n/2.0  # Murphy 2007, equ 88
    s = torch.var(datapoints, unbiased=False)
    beta_n = png.beta + (1/2) * ( n*s + png.kappa*n*((torch.mean(datapoints)-png.mu)**2) / (png.kappa + n) )
    # beta_n calculation see Murphy 2007, equ 89
    if torch.isnan(beta_n):
        raise ValueError("beta_n is nan, datapoints={}".format(datapoints))
    return NormalGamma(mu_n, kappa_n, alpha_n, beta_n)


def marginal_normal_gamma(normal_gamma):
    """computes the marginal distribution over the normal gamma distribution.
       Note it does not compute log_prob directly, but returns a distribution which can be used to query
       the log_prob of a dataset.
       myng = ng.NormalGamma(0.0, .01, 1.0, 1.0)
       datapoints = torch.tensor([3.4,7.8,9.8])
       marginal_normal_gamma(myng).log_prob(datapoints)
           >>>tensor([-3.5607, -3.9238, -4.1206])
    """
    # Murphy 2007, equ 100
    return torch.distributions.studentT.StudentT(
        df = torch.tensor(normal_gamma.alpha),
        loc=torch.tensor(normal_gamma.mu),
        scale=torch.sqrt(torch.tensor((normal_gamma.beta*(1+normal_gamma.kappa))/(normal_gamma.alpha*normal_gamma.kappa))))

# Reference:
# Kevin Murphy 2007, Conjugate Bayesian analysis of the Gaussian distribution
