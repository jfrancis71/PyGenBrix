
PyGenBrix is an experimental project for constructing generative probabilistic models with an emphasis on composability of probability distributions. It is aimed at vision applications.

Distribution Objects:
These support log_prob and sample methods.
log_prob actually returns a dictionary which will have a key "log_prob". They may return others keys which can be useful for logging purposes.
On construction they take parameters appropriate to that distribution, eg mean and std for normal distribution.
In the case of complicated distributions such as parallel cnn's, the constructor may need to take relevant neural networks as arguments to the constructor. Note the design decision that they do not store neural networks directly as a distribution object is created for each batch and a neural net would need to be shared across batches.

Summary: The intent is identical to PyTorch distributions, except log_prob returns dictionary with key "log_prob" in addition to other keys to enable logging for complicated distributions.

Layer Objects:
These implement the forward method and return associated Distribution objects. This design was based on the layer objects principles used by the TensorFlow Probability package.
They are intented to be used as the final layer of a neural network.
These may be trainable objects and if the associated Distribution object requires state eg neural network, then this is contained by the Layer object.

Generative Models:
These support log_prob and sample methods. (Also log_prob returning dictionary as in distribution objects)
Unlike distribution objects, they do not take any distribution-type parameters in their construction, and are not created on a per batch basis. It is intended that there is a single GenerativeModel per experiment, and that this is a trainable object.

PyGenBrixModel:
Is a convenience class which creates a trainable logits layer and feeds it into your Layer object of choice to create a Generative Model.

Example use:

>>mymodel = Train.PyGenBrixModel( 
>>    cnn.MultiStageParallelCNNLayer([ 1, 28, 28 ], vae.IndependentBernoulliLayer(), upsampling_stages = 1 ),
>>    [ 1, 28, 28 ],
>>    device )

We are moving to use PyTorch Lightning for training, so the model could be trained as follows:
>>pl.Trainer( fast_dev_run = False, gpus=1 ).fit( Train.LightningTrainer( mymodel, mnist_dataset, Train.disp, learning_rate = .001, batch_size = 16 ) )

Train.LightningTrainer is a convenience class for training models. It automatically splits the dataset into training and validation sets. It tracks the dictionaries containing logging information across batches and logs them to Tensorboard, along with 4x4 sample images and the PyTorch graph.
